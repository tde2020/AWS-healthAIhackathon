{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Building the model to classify electronic medical records (EMR)\n",
    "     - Use XGBoost to compare with previous Linear Learner model\n",
    "In the batch data processing using [HHBatchDataProcessing.ipynb](./HHBatchDataProcessing.ipynb), I have prepared a dataset by extracting medical records that had medical speciality of the following categories on the MTSamples data. Those records have been passed through Comprehend Medical to extract medical key workds and the data is converted to a flat file having the feature set and the label.\n",
    "\n",
    "    1: \"Cardiovascular / Pulmonary\"\n",
    "    2: \"Orthopedic\"\n",
    "    3: \"Radiology\"\n",
    "    4: \"General Medicine\"\n",
    "    5: \"Gastroenterology\"\n",
    "    6: \"Neurology\"\n",
    "\n",
    "\n",
    "In this notebook, I will be using the extracted dataset to create a classification model.\n",
    "\n",
    "The goal of this experiment is to do a **Next step Prediction** which aims at predicting the speciality needed for a patient with certain diseases. In practice, the model could be used to analyze a medical transcription in real-time that can be used to provide a recommended referals to respective specialist, provide medical information related to health condition, provide nutrition or suppliments, exercises or available therapies that can help to improve quality of life and life style decisions. In this way it can establish a portal to integrate health care providers to the patients. \n",
    "\n",
    "The input for the prediction is the EMR as a pdf file with doctor's notes about the patient or patients notes about their illness described in free form. This unstructured free form text is passed through Comprehend Medical to extract the medical terms which can then be used to predict medical speciality using the trained model.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Setup Environment](#Setup-Environment)\n",
    "1. [Load and Explore the Dataset](#Load-and-Explore-Dataset)\n",
    "1. [Prepare Dataset for Model Training](#Prepare-Dataset-for-Model-Training)\n",
    "1. [XGBoost Algorithm](#XGBoost-Algorithm)\n",
    "1. [Train the Model](#Train-the-Model)\n",
    "1. [Deploy and Evaluate the Model](#Deploy-and-Evaluate-the-Model)\n",
    "1. [Hyperparameter Optimization](#Hyperparameter-Optimization)\n",
    "1. [Inference Example](#Inference-Example)\n",
    "1. [Conclusion](#Conclusion)\n",
    "1. [Clean up resources](#Clean-up-resources)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective\n",
    "Predict health condition according to the EMR\n",
    "\n",
    "Input: Free text of patients health condition written by the patient, a prescription or a doctors transcript.\n",
    "\n",
    "Final goal: According to the predicted Health speciality, provide information about health recommendations and medical speciality.  (this programe is ending at the prediciton state but during a product implementation it can be integrated to a health care provider database which can provide information about illnesse, doctors list, nutrition or suppliment list, therapies etc.) \n",
    "\n",
    "Challenges:\n",
    "- Dataset is limited and a larger dataset will help to train the model with more accuracy.\n",
    "- Dataset contains limited amount of health conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Environment\n",
    "\n",
    "- **import** some useful libraries (as in any Python notebook)\n",
    "- **configure** the S3 bucket and folder where data should be stored (to keep our environment tidy)\n",
    "- **connect** to AWS in general (with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)) and SageMaker in particular (with the [sagemaker SDK](https://sagemaker.readthedocs.io/en/stable/)), to use the cloud services\n",
    "- **Upgrade** SageMaker to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textract-trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For matrix operations and numerical processing\n",
    "import pandas as pd  # For munging tabular data\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# import self-defined functions\n",
    "#from util.classification_report import generate_classification_report, predict_from_numpy_V2  # helper function for classification reports\n",
    "from util.Pipeline import extractTextract, extractMedical\n",
    "from util.preprocess import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.amazon.amazon_estimator import RecordSet\n",
    "\n",
    "# setting up SageMaker parameters\n",
    "import pkg_resources\n",
    "pkg_resources.require(\"sagemaker>2.9.2\") \n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"emr-mtSample\"  # Location in the bucket to store our files\n",
    "sgmk_session = sagemaker.Session()\n",
    "sgmk_client = boto_session.client(\"sagemaker\")\n",
    "sgmk_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Explore Dataset\n",
    "\n",
    "Load the dataset prepared from the previous notebook [BatchDataProcessing](./BatchDataProcessing.ipynb). This dataset contains labelled data based on the medical speciality selected above and the medical features that were extracted from the electronic medical reports.\n",
    "\n",
    "You can find the processed dataset in the following location '/data/processed_combined_extract.csv'.\n",
    "\n",
    "*Demographics:*\n",
    "* `ID`: id of the patients (int)\n",
    "* `Label`: the medical condition (1-6 chosen categories)\n",
    "* The rest of the columns e.g. `fever`, `wheezing`: medical condition extracted from notes. The number indicate confidence of the symptom (float), there are 113 features in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_full=pd.read_csv(\"./data/processed_combined_extract.csv\")\n",
    "df_wide_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore correlation between the input variables and output one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrPlot(df_wide_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare-Dataset-for-Model-Training\n",
    "\n",
    "1. Suffle and split the data into **Training (80%)**, **Validation (10%)**, and **Test (10%)** sets\n",
    "2. Convert the data to the format the algorithm expects (e.g. CSV)\n",
    "3. Upload the data to S3\n",
    "4. Create `s3_input` objects defining the data sources for the SageMaker SDK\n",
    "\n",
    "The training and validation datasets will be used during the training (and tuning) phase, while the 'holdout' test set will be used afterwards to evaluate the model.\n",
    "\n",
    "SageMaker XGBoost algorithm expects data in the **libSVM** or **CSV** formats with the following format:\n",
    "- The target variable in the first column, and\n",
    "- No header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost data prep\n",
    "# remove the id column \n",
    "df_combined_model=df_wide_full.iloc[:,1:] \n",
    "\n",
    "# transform labels to 0 index to compare with the LL model results\n",
    "df_wide_full['Label'] -= 1\n",
    "\n",
    "# all feature data should be float32\n",
    "df_wide_full=df_wide_full.apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "\n",
    "# Shuffle and splitting dataset\n",
    "train_data, validation_data, test_data = np.split(df_combined_model.sample(frac=1, random_state=123), \n",
    "                                                  [int(0.8 * len(df_combined_model)), int(0.9*len(df_combined_model))],) \n",
    "\n",
    "# Create CSV files for Train / Validation / Test\n",
    "train_data.to_csv(\"data/train.csv\", index=False, header=False)\n",
    "validation_data.to_csv(\"data/validation.csv\", index=False, header=False)\n",
    "test_data.to_csv(\"data/test.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG boost\n",
    "# Upload CSV files to S3 for SageMaker training\n",
    "train_uri = sgmk_session.upload_data(\n",
    "    path=\"data/train.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=bucket_prefix\n",
    ")\n",
    "val_uri = sgmk_session.upload_data(\n",
    "    path=\"data/validation.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=bucket_prefix\n",
    ")\n",
    "\n",
    "# Create s3_inputs\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data=train_uri, content_type=\"csv\")\n",
    "s3_input_validation = sagemaker.TrainingInput(s3_data=val_uri, content_type=\"csv\")\n",
    "\n",
    "print(f\"{s3_input_train.config}\\n\\n{s3_input_validation.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Algorithm\n",
    "**`XGBoost`** stands for e**X**treme **G**radient **Boosting**. It implements the gradient boosting decision tree algorithm, which is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction.\n",
    "\n",
    "The two major advantages of using XGBoost are:\n",
    "\n",
    "    1. Fast Execution Speed: Generally, XGBoost is faster when compared to other implementations of gradient boosting.\n",
    "    2. High Model Performance: XGBoost has exceled in either structured or tabular datasets on classification and regression predictive modeling problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost\n",
    "from sagemaker import image_uris \n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.5-1\") # fot multi class\n",
    "\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters & Algorithm\n",
    "* image_name - Training image to use(image_name), in this case we will be using the xgboost training image\n",
    "* train_instance_type - Type of instance to use.\n",
    "* train_instance_count - The number of instances to run the training job. For suitable algorithms that support distributed training, set an instance count of more than 1.\n",
    "* role - IAM role used to run the training job\n",
    "* train_use_spot_instances - Specify whether to use spot instances. \n",
    "* train_max_run - Timeout in seconds for training (default: 24 * 60 * 60). After this amount of time Amazon SageMaker terminates the job regardless of its current status.\n",
    "* train_max_wait - Timeout in seconds waiting for spot training instances\n",
    "* hyperparameters - Our hyperparameters used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"num_round\": \"150\",     # int: [1,300]\n",
    "    \"max_depth\": \"6\",     # int: [1,10]\n",
    "    \"alpha\": \"2.5\",         # float: [0,5]\n",
    "    \"eta\": \"0.2\",           # float: [0,1]\n",
    "#    \"objective\": \"binary:logistic\", # binary classification\n",
    "    \"objective\": \"multi:softmax\",    # multi class\n",
    "    \"num_class\": \"8\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "}\n",
    "\n",
    "# Instantiate an XGBoost estimator object\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=training_image,           # XGBoost algorithm container\n",
    "    instance_type=\"ml.m5.xlarge\",  # type of training instance\n",
    "    instance_count=1,              # number of instances to be used\n",
    "    role=sgmk_role,                      # IAM role to be used\n",
    "    use_spot_instances=True,       # Use spot instances to reduce cost\n",
    "    max_run=20*60,                 # Maximum allowed active runtime\n",
    "    max_wait=30*60,                # Maximum clock time (including spot delays)\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train-the-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start a training (fitting) job\n",
    "estimator.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Evaluate the Model\n",
    "After trainin the model, proceed with deploying the model (hosting it behind a real-time endpoint) so that we can start running predictions in real-time. This can be done using the `estimator.deploy()` function. (https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html.)\n",
    "\n",
    "This deployment might take few minutes, and by default the code will wait for the deployment to complete.\n",
    "\n",
    "+ Use the Endpoints page of the SageMaker Console to check the status of the deployment\n",
    "+ You can start the Hyperparameter Optimization job in parallel - which will take a while to run too. \n",
    "+ Prediction would have to wait till the end point deployment is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    endpoint_name='hhxgbmulti',\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    #inference_response_keys=inference_response_keys,\n",
    "    predictor_cls=sagemaker.predictor.Predictor,\n",
    "    #serializer = sagemaker.serializers.CSVSerializer()\n",
    "    #wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "\n",
    "Once the Sagemaker endpoint has been deployed, we can now run some prediction to test our endpoint. Let us test our endpoint by running some predictions on our test data and evaluating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for test data\n",
    "resultxgb=predictor.predict(test_data.iloc[:,1:].values)\n",
    "print(resultxgb) # this result is in byte format\n",
    "#print(test_data.iloc[:,0:1].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=list(resultxgb.decode()[0:-1].split('\\n')) # split to get predicted labels\n",
    "print(pred)\n",
    "df_pred=pd.DataFrame(pred) # convert to dataframe\n",
    "df_pred=df_pred.apply(pd.to_numeric, downcast='float', errors='coerce') # convert to float\n",
    "print(df_pred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: \"Cardiovascular / Pulmonary\",\n",
    "    1: \"Orthopedic\",\n",
    "    2: \"Radiology\",\n",
    "    3: \"General Medicine\",\n",
    "    4: \"Gastroenterology\",\n",
    "    5: \"Neurology\",\n",
    "}\n",
    "label_mapper = np.vectorize(lambda x: label_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# classification report \n",
    "print(\"Label category (1-5):\", list(label_mapper(list(label_map.keys()))))\n",
    "print(classification_report(test_data.iloc[:,0:1].values, df_pred. values,labels=list(label_map.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model from the training job\n",
    "\n",
    "After the training job is done, the model is not saved yet. Check training jobs and models in your SageMaker Console. To create a model from a training job, refer to the documentation for  *[create_model API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a primary container with the trained model \n",
    " model_data=estimator.create_model().model_data\n",
    " primary_container = {\n",
    "     'Image': training_image,\n",
    "     'ModelDataUrl': model_data\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare a model for hosting to run inference\n",
    " create_model_response = sgmk_client.create_model(\n",
    "     ModelName = 'hhxgbmulti',\n",
    "     ExecutionRoleArn = sgmk_role,\n",
    "     PrimaryContainer = primary_container,\n",
    " )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameter Optimization - TODO - NOT TESTED!!\n",
    "\n",
    "We can check if the model improves with SageMaker HyperParameter Optimization (HPO) by automating the search for an optimal hyperparameter. We **specify a range**, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# set up hyperparameter ranges\n",
    "ranges = {\n",
    "    \"num_round\": IntegerParameter(100, 300),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 5),\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "}\n",
    "\n",
    "# set up the objective metric\n",
    "objective = \"validation:auc\"\n",
    "#objective = \"validation:accuracy\"\n",
    "# instantiate a HPO object\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,              # the SageMaker estimator object\n",
    "    hyperparameter_ranges=ranges,     # the range of hyperparameters\n",
    "    max_jobs=10,                      # total number of HPO jobs\n",
    "    max_parallel_jobs=2,              # how many HPO jobs can run in parallel\n",
    "    strategy=\"Bayesian\",              # the internal optimization strategy of HPO\n",
    "    objective_metric_name=objective,  # the objective metric to be used for HPO\n",
    "    objective_type=\"Maximize\",        # maximize or minimize the objective metric\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# start HPO\n",
    "tuner.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wait, until HPO is finished\n",
    "hpo_state = \"InProgress\"\n",
    "\n",
    "while hpo_state == \"InProgress\":\n",
    "    hpo_state = sgmk_client.describe_hyper_parameter_tuning_job(\n",
    "                HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)[\"HyperParameterTuningJobStatus\"]\n",
    "    print(\"-\", end=\"\")\n",
    "    time.sleep(60)  # poll once every 1 min\n",
    "\n",
    "print(\"\\nHPO state:\", hpo_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# deploy the best model from HPO\n",
    "hpo_predictor = tuner.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\",predictor_cls=sagemaker.predictor.Predictor,\n",
    "    serializer = sagemaker.serializers.CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hpo_predictor.deserializer=sagemaker.deserializers.CSVDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predicted probabilities of the best model\n",
    "hpo_predictions = predict_from_numpy_V2(hpo_predictor, test_data.drop([\"Label\"], axis=1))\n",
    "print(hpo_predictions)\n",
    "\n",
    "# generate report for the best model\n",
    "generate_classification_report(\n",
    "    y_real=test_data[\"Label\"].values, \n",
    "    y_predict_proba=hpo_predictions, \n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"Consultation\",\"Surgery\"],\n",
    "    title=\"Best model (with HPO)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Inference Example\n",
    "\n",
    "A simplified pipeline to process an Electronic Health Record\n",
    "Combine Textract, Comprehend Medical and SageMaker endpoint to process an electronic medical resport. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "from util.Pipeline import extractTextract, extractMedical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract data from Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFprefix='hhtestdata' # bucket name if you use test data from s3- customize to your s3 if you test this code\n",
    "\n",
    "# Check the 2 use cases seperately (you should chose either Use case 1 or Use case 2\n",
    "# If you chose use case 1, you can skip the next few blocks and directly go to Step 2: Extract data from Comprehend Medical\n",
    "# Use case 1 - English language report\n",
    "#fileName =  'sample_report_1.pdf' \n",
    "\n",
    "# Use case 2 - German language report\n",
    "fileName =  'sample_report_2.pdf' \n",
    "\n",
    "fileUploadPath = os.path.join(\"./data\", fileName) # if you upload from working dir\n",
    "#fileUploadPath = os.path.join(PDFprefix, fileName) # if you upload from a s3 bucket\n",
    "print(\"EHR file to be processed is at \", fileUploadPath)\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket_name).Object(fileName).upload_file(\n",
    "    fileUploadPath\n",
    ")\n",
    "\n",
    "doc=extractTextract(bucket_name, fileName) # extract pdf file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # read full text\n",
    "print(\"Total length of document is\", len(doc.pages))\n",
    "idx = 1\n",
    "full_text = \"\"\n",
    "for page in doc.pages:\n",
    "    print(f\"Results from page {idx}: \\n\", page.text)\n",
    "    full_text += page.text\n",
    "    idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect languagge\n",
    "comprehend_client = boto3.client(service_name=\"comprehend\", region_name=\"us-east-1\")\n",
    "response = comprehend_client.detect_dominant_language(Text=full_text).get(\n",
    "    \"Languages\", []\n",
    ")\n",
    "for language in response:\n",
    "    print(\n",
    "        f\"Detected language is {language.get('LanguageCode', [])}, with a confidence score of {language.get('Score', [])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if language is de then translate to en\n",
    "if language.get('LanguageCode', [])=='de':\n",
    "    translate = boto3.client(service_name='translate', region_name='us-east-1', use_ssl=True)\n",
    "    result = translate.translate_text(Text=full_text[:5000], SourceLanguageCode=\"de\", TargetLanguageCode=\"en\")\n",
    "    enFullText = result.get('TranslatedText')\n",
    "    print('TranslatedText: ' + enFullText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract data from Comprehend Medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language.get('LanguageCode', [])=='de':\n",
    "    comprehend_medical_client = boto3.client(service_name='comprehendmedical', region_name='us-east-1')\n",
    "    comprehendResponse = comprehend_medical_client.detect_entities_v2(Text=enFullText)\n",
    "    df_cm=extractMC_v2(comprehendResponse) # create dataframe with feature set\n",
    "else:\n",
    "    comprehendResponse=extractMedical(doc)\n",
    "    df_cm=extractMC_v2(comprehendResponse[0]) # create dataframe with feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Organize the extracted json file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclist, df_cm2=retrieve_mcList(df_cm, nFeature=40,threshold=0.8) # use same nfeatures and threshold as before\n",
    "df_cm2=df_mc_generator_slim(df_cm2)\n",
    "df_cm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prediction with the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataset with same feature list as in our dataset used to train\n",
    "df_final=test_features.iloc[0:0,0:] \n",
    "#print(df_final)\n",
    "\n",
    "# chose from the comprehend medical extracted features only features as in the train dataset\n",
    "df_final=df_final.append(df_cm2[df_cm2.columns.intersection(df_final.columns)])\n",
    "\n",
    "df_final=df_final.fillna(0)\n",
    "df_final=df_final.apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "#print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. predict with trained model\n",
    "result=predictor.predict(df_final.values)\n",
    "# result=LL-HH-model.predict(df_final.values) # using setup model\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv format input string to predict using endpoint\n",
    "import json\n",
    "#print(df_final.values)\n",
    "s = json.dumps(df_final.values.tolist())\n",
    "#print(s[0:])\n",
    "td=s[0:]\n",
    "td=td.replace('[', '')\n",
    "td=td.replace(']', '')\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#td='0.0, 0.0, 0.0, 0.0, 0.0, 0.6807340383529663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9984696507453918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0'\n",
    "#td-de=0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7928379774093628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "# predict with new model using setup endpoint\n",
    "# xgbmulticlass\n",
    "endpoint = 'hhxgbmulti'\n",
    "runtime = boto3.Session().client('sagemaker-runtime')\n",
    "# Send image via InvokeEndpoint API\n",
    "responsexgb = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='text/csv', Body=td)\n",
    "\n",
    "# Unpack response\n",
    "resultxgb = json.loads(responsexgb['Body'].read().decode())\n",
    "print(resultxgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "According to the comparison with the previous model:\n",
    "LL accuracy was 0.55 in average and 0.68 for best category and XGBoost accuracy using default parameters was 0.51 & best category 0.61.\n",
    "\n",
    "**Parameter tuning and model testing yet to do - WIP!!!**\n",
    "\n",
    "At the end inference results showed the predicted classification which can be used for providing health recommendations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean up resources\n",
    "### Delete the endpoint and configuration if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "hpo_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the generated files S3 bucket files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete all the content in the emr-mtSample folder. Check S3 before deleting it\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.objects.filter(Prefix=bucket_prefix).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete all the content in the PDF folder \n",
    "\n",
    "bucket.objects.filter(Prefix=PDFprefix).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice:\n",
    " 1. Delete the buckets created from testing\n",
    " 2. Shut down your notebook instance if you are not planning to explore more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
